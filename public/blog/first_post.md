# Emergent Geometry Inside Neural Networks

**Table of Contents**  
- [Introduction: The Shapes Within](#introduction-the-shapes-within)  
- [Polyhedral Partitions and Monosemantic Regions](#polyhedral-partitions-and-monosemantic-regions)  
- [Manifolds and Topological Patterns in Representations](#manifolds-and-topological-patterns-in-representations)  
- [Circular Features and Modular Arithmetic](#circular-features-and-modular-arithmetic)  
- [Simplex Structures and Neural Collapse](#simplex-structures-and-neural-collapse)  
- [Discussion: Interpreting Geometry for Insight](#discussion-interpreting-geometry-for-insight)  

## Introduction: The Shapes Within

Deep neural networks transform data through high-dimensional spaces. A recurring theme in **interpretability research** is that complex internal representations often exhibit **emergent geometric structures**. These may include clusters of related concepts, polyhedral partitions created by activation boundaries, low-dimensional manifolds of features, and even **circular arrangements of specific features** in latent space. Recent work by **Anthropic and others** has aimed to “open the black box” by mapping such internal structures. Understanding these shapes is more than theoretical – it sheds light on *how models organize knowledge*, and could help make AI systems safer and more predictable.

This post surveys recent research (circa 2024–2025) on emergent shapes in neural network representations. We’ll cover the **polytope lens** on ReLU networks, the role of **manifolds** and **topological loops** in feature space, evidence of **polyhedra and clusters** in activation patterns, and surprising examples like features arranged on a **circle**. Throughout, we aim to combine technical detail with intuitive analogies to guide the reader through these conceptual landscapes.

## Polyhedral Partitions and Monosemantic Regions

Modern neural networks often use piecewise linear activation functions (like ReLU), which partition the input space into **polytopes** – polyhedral regions where the network’s behavior is linear. Researchers have proposed interpreting networks through a **polytope lens**, positing that the fundamental “units” of a ReLU network’s representation might be these polytopal regions rather than individual neurons ([[2211.12312] Interpreting Neural Networks through the Polytope Lens](https://arxiv.org/abs/2211.12312#:~:text=In%20order%20to%20find%20a,density%20of%20polytope%20boundaries%20reflect)). Each region corresponds to a specific pattern of activation (with certain neurons “on” and others “off”), effectively carving up the high-dimensional space into many cells. Within a single polytope, the network’s output is an affine function of the input, and crossing a polytope boundary corresponds to a neuron’s activation turning on/off.

**Why look at polytopes?** A key observation is that directions in activation space (like individual neuron axes or linear combinations thereof) can be *polysemantic* – i.e., they mix multiple unrelated concepts ([[2211.12312] Interpreting Neural Networks through the Polytope Lens](https://arxiv.org/abs/2211.12312#:~:text=individual%20neurons%20or%20their%20linear,study%20the%20way%20that%20piecewise)). However, when we zoom in to consider an individual polytope region, we often find it represents a more coherent concept. In other words, *each linear region can be “monosemantic” even if a direction is not*. Black et al. (2022) demonstrated that one can identify **monosemantic regions of activation space** using the polytope lens ([[2211.12312] Interpreting Neural Networks through the Polytope Lens](https://arxiv.org/abs/2211.12312#:~:text=linear%20activation%20functions%20,like%20through%20the%20polytope%20lens)). For example, in a vision classifier, one polytope might correspond to “dog-like features present” while another corresponds to “no dog features.” If you stay within a given polytope, the meaning of activations remains consistent, avoiding neuron-level ambiguity.

Moreover, the **density of polytope boundaries** often correlates with semantic boundaries: regions of feature space with many classification boundaries typically align with changes in the data’s class or concept ([[2211.12312] Interpreting Neural Networks through the Polytope Lens](https://arxiv.org/abs/2211.12312#:~:text=linear%20activation%20functions%20,like%20through%20the%20polytope%20lens)). This is intuitive – a complex decision boundary (many polytope facets) often lies between different classes. Thus, analyzing polytope geometry gives a *spatial map* of where the network distinguishes between categories. By outlining which polytopes map to which concepts, interpretability researchers are effectively identifying **geometric decision circuits** within the model.

## Manifolds and Topological Patterns in Representations

Another perspective comes from viewing network activations as living on or near a **low-dimensional manifold** inside the high-dimensional space. The **manifold hypothesis** suggests that real-world data (and by extension, internal representations of that data) often occupy a constrained, nonlinear manifold rather than filling the entire ambient space. Recent topological data analysis (TDA) studies have begun probing these manifolds within neural networks to identify notable shapes like clusters, loops, and voids.

For instance, a 2022 study explored the **topology of data manifolds across layers** in convolutional networks. Early in the network, data representations (say of different image classes) might be highly tangled. As the network’s layers increase, these manifolds tend to **untangle and flatten**, making classes more separable (think of a twisted cluster of points gradually morphing into a separated, compact cluster). This is consistent with the idea that deep networks turn complicated, curved manifolds (e.g. images of cats vs. dogs) into **linearly separable** forms by the final layer. The process involves *continuous deformation*: stretching, compressing, and shearing the manifold without tearing it (much like a topologist’s transformations). By the final layers, one can often separate classes with a simple hyperplane, indicating the manifold of each class has become almost convex or at least distinctly isolated in space.

**Topological descriptors** like **Betti numbers** and **persistent homology** provide tools to quantify these shapes. Betti numbers $b_0, b_1, b_2, \dots$ count topological features (connected components, loops, cavities, etc.). Through layer-wise analysis, researchers found that as data passes through layers, the number of loops or independent components can change, reflecting the model’s progressive reorganization. For example, a loop in the data manifold might correspond to a continuously varying attribute (imagine images of a rotated object – the rotation angle might form a loop structure in representation space). Early layers might preserve that loop, but deeper layers could break or simplify it if the rotation is irrelevant to the final task.

A particularly striking example of a topological pattern is when features themselves form **circular structures**. *If you only consider deformations that preserve topology (like rubber-sheet stretching), a circle is distinct from a line or cluster because it has a 1-dimensional hole.* This insensitivity to bending is why topology can reveal a “loop” where normal geometry might just see a set of points. An **Anthropic interpretability team** post by Carlsson (2025) highlights this: some language model **features are organized in a circular pattern** in feature space. The loop structure was not random; it corresponded to inherently **periodic data** – in this case, temporal cycles like days of the week or months of the year. 

## Circular Features and Modular Arithmetic

One of the most intriguing recent discoveries is that certain high-dimensional features correspond to **modular concepts** that naturally form loops or circles in activation space. Engels et al. (2024) and follow-up works found that language models learned *multi-dimensional features* for **days of the week** and **months of the year**, whose activations lie on a circle. Intuitively, the model seems to understand that after Sunday comes Monday (looping back around), or after December comes January, forming a cyclical continuum. This is a concrete example of an **irreducible two-dimensional feature**: you need two dimensions to represent the cyclic order, as any single scalar projection would break the loop (a 1D line can’t seamlessly loop back on itself, but a circle can).

These *circular features* were identified via **sparse autoencoders (SAEs)** that “decode” internal activations into a set of human-interpretable feature directions. By performing PCA (principal component analysis) on the learned feature vectors for dates, researchers visualized a **ring-like structure**. In the case of weekdays, smaller models or fewer features yielded a single “weekday” direction that roughly activates on all days. But as models scale up (and the SAE captures more features), that general concept splits into finer ones: individual features for Monday, Tuesday, etc. Plotting these, the days arrange around a circle in the plane spanned by the top two principal components. 

 ([Calendar feature geometry in GPT-2 layer 8 residual stream SAEs — AI Alignment Forum](https://www.alignmentforum.org/posts/WsPyunwpXYCM2iN6t/calendar-feature-geometry-in-gpt-2-layer-8-residual-stream)) *Figure 1: PCA projection of **weekday features** from GPT-2’s layer 8 (residual stream) features. Each point is a feature discovered by an SAE, colored by model/feature set size, and labeled if it corresponds to a specific day. Notice how days of the week cluster and arrange in a loop (e.g. Sunday points neighbor Monday, etc.), reflecting the cyclic nature of the calendar.* 

Similarly, **months of the year** form a loop in feature space. January comes after December, and indeed the learned “December” feature lies adjacent to “January” when visualized. The **Month features PCA** below shows this roughly circular progression through the calendar months (with perhaps some distortion) across various model scales:

 ([Calendar feature geometry in GPT-2 layer 8 residual stream SAEs — AI Alignment Forum](https://www.alignmentforum.org/posts/WsPyunwpXYCM2iN6t/calendar-feature-geometry-in-gpt-2-layer-8-residual-stream)) *Figure 2: PCA projection of **month features** across various SAE scales. Each point corresponds to a feature activating on a specific month (colored by SAE size, labeled for single-month features). The arrangement reveals a loop: December and January are adjacent (upper left), and the sequence wraps around through the year. Larger models tend to allocate one feature per month, whereas smaller ones merge months.* 

Why do such circles emerge? It appears the model internally represents these temporal concepts using a form of **modular arithmetic**. To correctly compute “the next day” or determine that Wednesday is two days after Monday, the network benefits from a representation where moving along a continuous direction corresponds to advancing through days – and after Sunday, you wrap around to Monday. Indeed, Engels et al. (2025) showed that these circular features are *used by the model* to solve tasks like computing the day $n$ days ahead (a kind of addition modulo 7). When they intervened on these features (in models like GPT-2 or Mistral), they could disrupt or alter the model’s computation on date tasks, confirming that **the circle is a real computational structure, not just a visual quirk**.

Mathematically, one can imagine two neurons whose activations form a 2D plane encoding a rotation: e.g., represent day $d$ as $(\cos(2\pi d/7), \;\sin(2\pi d/7))$. This 2D representation is inherently circular (rotating by $2\pi/7$ moves one day forward). A linear combination (one-dimensional direction) couldn’t capture this cleanly because there’s no single line in the plane that you can slide along and treat Monday and Sunday as neighbors without discontinuity. The model *learned something like a sinusoidal code* for days and months, which is remarkable evidence of it discovering a **topologically nontrivial representation** on its own.

## Simplex Structures and Neural Collapse

Not all emergent geometry is nonlinear or topological – some of it is surprisingly symmetric and polyhedral. In supervised classifiers, especially as they reach the end of training, a phenomenon called **Neural Collapse (NC)** has been observed. Neural Collapse refers to several related properties in the final-layer features of a network (just before the classifier logits):

- **NC1:** *Within-class variability collapses*, meaning all examples from the same class have nearly identical feature representations (they cluster tightly around the class mean).  
- **NC2:** *Between-class means collapse to a Simplex Equiangular Tight Frame (ETF)*. This is a fancy way of saying that the class means become equally spaced **vertices of a simplex** in feature space. For $C$ classes, their means $\mu_1, \mu_2, \dots, \mu_C$ satisfy $\|\mu_i\| = \|\mu_j\|$ for all $i,j$ (equal norm) and $\langle \mu_i, \mu_j \rangle$ is constant for all $i\neq j$ (equal angle between any pair). Geometrically, they behave like $C$ points on a sphere that are as far apart from each other as possible – which is exactly a regular simplex configuration in high dimensions.  
- **NC3:** *Classifier vectors collapse to align with these means*, and classification essentially becomes “choose nearest class mean.”  

In plain terms, by the end of training, the network learns an internal encoding where each class is represented by a distinct **direction** in the final-layer feature space, and those directions are maximally separated (forming a simplex arrangement). This is an emergent property, not something explicitly imposed by the loss function (though cross-entropy and MSE seem to encourage it naturally). It’s stunning because it implies a high level of **symmetry** and efficiency: the network has carved the feature space into $C$ sectors (one per class) with nice geometric properties.

The simplex ETF structure is beneficial for both **interpretability and generalization**. Interpretability improves because each class has a clear geometric signature – an example’s class can be determined by which simplex vertex it’s closest to. Additionally, errors or adversarial perturbations might need to overcome that large angular separation between classes, hinting at increased robustness. Several works in 2023–2024 have built on neural collapse: some aim to enforce it earlier in training or in imbalanced scenarios, while others examine how intermediate layers also begin to show **partial collapse** phenomena where subclasses or semantic groupings form their own simplexes.

It’s worth noting that neural collapse aligns with the earlier idea of *manifold flattening*. If each class’s manifold is squeezed down to essentially a point (or a very tight cluster), and those points happen to form a simplex, the model has achieved a very organized separation. In practice, real networks might not reach the perfect simplex ETF due to limited data or early stopping, but they often **trend toward it**. Empirical studies confirm that during late training stages, within-class variance shrinks and class means equiangularly spread out.

To connect this to our broader theme: a simplex is a **geometric shape** (in $n$ dimensions, an $n$-simplex has $n+1$ vertices). In high dimensions, a regular simplex is the most symmetric way to arrange a given number of points on a sphere. Neural networks *discover this configuration* as a byproduct of training, hinting at an underlying geometric optimality in representations.

## Discussion: Interpreting Geometry for Insight

The emergence of geometric structures in neural network representations – from polytopes and clusters to loops and simplexes – provides a powerful language for **interpretability**:

- **Polyhedral View:** By analyzing which region of activation space an input lies in, we can infer what combination of *nonlinear conditions* (neurons on/off) the model is using. This relates to *rule-like interpretations*: each polytope could be seen as a different *case* or context the network considers, with its own linear rule ([[2211.12312] Interpreting Neural Networks through the Polytope Lens](https://arxiv.org/abs/2211.12312#:~:text=linear%20activation%20functions%20,like%20through%20the%20polytope%20lens)). Mechanistic interpretability research imagines mapping entire circuits by following how polytopes connect and where boundaries lie (essentially reverse-engineering piecewise linear rules).

- **Manifold and Topology:** Understanding the intrinsic manifold of data through the network helps explain *generalization*. If two concepts are similar (e.g., two breeds of dogs), their representations will lie nearby on the manifold. Topological features like loops might indicate model usage of continuous cyclic variables (like rotation, time of day, etc.). By identifying such loops, one can guess the model has learned a periodic function. Carlsson’s example of distinguishing a loop (like an “8” shape having a hole) is a handy reminder: networks might treat `0` and `8` differently because one has a topological hole and the other doesn’t. In practice, features sensitive to “has a loop” are detected in vision models (some neurons fire for `8` and `0` differently due to that extra enclosed area, analogous to topology’s perspective).

- **Circular Features:** These show that some internal computations are not linear but *cyclical*. It’s an insight into how models may represent state-like or modular information (dates, clock times, list positions, etc.). When such features are discovered, they can sometimes be manipulated. For example, one could try to shift the phase of the circle to trick the model’s date calculations (this is an active area of research: intervening in feature space to edit model behavior).

- **Clusters and Simplexes:** When a model forms clear clusters for concepts or classes, it becomes easier to interpret *concept activation vectors* or use techniques like **Activation Atlases** to visualize what each cluster represents. The simplex ETF result is especially elegant – it tells us that, at convergence, the network has arranged concepts in a remarkably structured way. This could inspire new regularization methods or architectures that encourage interpretability from the get-go.

In closing, the study of shapes, manifolds, and geometry in neural networks serves as a bridge between abstract high-dimensional algebra and human-intuitive spatial reasoning. By mapping a model’s mind onto charts and diagrams, we gain intuition about *why* a model made a certain decision. This line of research is quickly evolving: every few months, new papers (often on arXiv or from teams like Anthropic’s interpretability group) reveal another facet of these hidden structures – perhaps polyhedra in GPT attention, or torus-like shapes in recurrent dynamics. The hope is that by charting these internal geometries, we not only make models more transparent but also uncover fundamental principles of representation learning. The shapes within might just be the key to understanding the **conceptual geometry of thought** inside our latest AI systems.

**Sources:** Recent research and articles including Anthropic interpretability reports and arXiv papers have informed this summary, especially on dictionary learning for feature extraction, the polytope lens perspective ([[2211.12312] Interpreting Neural Networks through the Polytope Lens](https://arxiv.org/abs/2211.12312#:~:text=linear%20activation%20functions%20,like%20through%20the%20polytope%20lens)), topological data analysis for loops, circular day/month features, and neural collapse simplex structure. These works exemplify the merging of deep learning with geometry and topology to illuminate the inner workings of neural nets.